{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.GraphKeys.VARIABLES = tf.GraphKeys.GLOBAL_VARIABLES\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "import shutil\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def inception1(input_tensor):\n",
    "    ## 2*2\n",
    "    conv2_2 = tf.layers.conv2d( input_tensor,filters = 128, kernel_size = 2, strides = 1,padding = \"same\",\n",
    "                                    activation = tf.nn.relu, use_bias = True, \n",
    "                                    kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                    bias_initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    conv2 = tf.layers.conv2d( conv2_2,filters = 128, kernel_size = 2, strides = 1,padding = \"same\",\n",
    "                                    activation = tf.nn.relu, use_bias = True, \n",
    "                                    kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                    bias_initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    ## 3*3\n",
    "    conv3_3 = tf.layers.conv2d( input_tensor,filters = 128, kernel_size = 3, strides = 1,padding = \"same\",\n",
    "                                    activation = tf.nn.relu, use_bias = True, \n",
    "                                    kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                    bias_initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    conv3 = tf.layers.conv2d( conv3_3,filters = 128, kernel_size = 3, strides = 1,padding = \"same\",\n",
    "                                    activation = tf.nn.relu, use_bias = True, \n",
    "                                    kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                    bias_initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    ##5*5\n",
    "    conv5_5 = tf.layers.conv2d( input_tensor,filters = 128, kernel_size = 5, strides = 1,padding = \"same\",\n",
    "                                activation = tf.nn.relu, use_bias = True, \n",
    "                                kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                bias_initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    conv5 = tf.layers.conv2d( conv5_5,filters = 128, kernel_size = 5, strides = 1,padding = \"same\",\n",
    "                                activation = tf.nn.relu, use_bias = True, \n",
    "                                kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                bias_initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    \n",
    "    combine_tensor1 = tf.concat([input_tensor,conv2],2)\n",
    "    combine_tensor2 = tf.concat([conv3, conv5],2)\n",
    "    output_tensor = tf.concat([combine_tensor1, combine_tensor2],1)\n",
    "    \n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,shape = [None,13*13*9])\n",
    "y_ = tf.placeholder(tf.int64,shape = [None])\n",
    "y_one_hot = tf.one_hot(y_, 2)\n",
    "x_image = tf.reshape(x,[-1,13,13,9])\n",
    "train_key = tf.placeholder(bool)\n",
    "with tf.variable_scope(\"PHM\"):\n",
    "    ## siganl preprocess\n",
    "    result_conv1 = tf.layers.conv2d( x_image,filters = 16, kernel_size = 1, strides = 1,padding = \"same\",\n",
    "                                    activation = tf.nn.relu, use_bias = True,\n",
    "                                    kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                    bias_initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    result_conv2 = tf.layers.conv2d( result_conv1,filters = 32, kernel_size = 1, strides = 1,padding = \"same\",\n",
    "                                    activation = tf.nn.relu, use_bias = True, \n",
    "                                    kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                    bias_initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    \n",
    "    result_conv3 = tf.layers.conv2d( result_conv2,filters = 64, kernel_size = 1, strides = 1,padding = \"same\",\n",
    "                                    activation = tf.nn.relu, use_bias = True, \n",
    "                                    kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                    bias_initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
    "\n",
    "    result_conv4 = tf.layers.conv2d( result_conv3,filters = 128, kernel_size = 1, strides = 1,padding = \"same\",\n",
    "                                    activation = tf.nn.relu, use_bias = True, \n",
    "                                    kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                    bias_initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    result_conv5 = inception1(result_conv4)\n",
    "\n",
    "    result_conv5_1 =  tf.layers.conv2d( result_conv5,filters = 176, kernel_size = 1, strides = 1,padding = \"same\",\n",
    "                                    activation = tf.nn.relu, use_bias = True, \n",
    "                                    kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                    bias_initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
    " \n",
    "    result_conv6 = tf.layers.conv2d(result_conv5_1,filters = 194, kernel_size = 13, strides = 13,padding = \"valid\",\n",
    "                                    activation = tf.nn.relu, use_bias = True, \n",
    "                                    kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                    bias_initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
    "\n",
    "\n",
    "   \n",
    "    result_conv6_1 = tf.layers.conv2d( result_conv6,filters = 256, kernel_size = 1, strides = 1,padding = \"same\",\n",
    "                                    activation = tf.nn.relu, use_bias = True, \n",
    "                                    kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                    bias_initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    \n",
    "    result_conv6_1_1 = tf.layers.conv2d( result_conv6_1,filters = 512, kernel_size = 1, strides = 1,padding = \"same\",\n",
    "                                    activation = tf.nn.relu, use_bias = True, \n",
    "                                    kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                    bias_initializer = tf.contrib.layers.xavier_initializer_conv2d())    \n",
    "    \n",
    "    \n",
    "    result_conv6_flat = tf.reshape( result_conv6_1_1 , [-1, 2 * 2 * 512])\n",
    "    \n",
    "    \n",
    "    result_dense6 = tf.layers.dense(inputs= result_conv6_flat, units = 1024, activation=tf.nn.relu, use_bias=True,\n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                    bias_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    result_dense6_drop = tf.layers.dropout(result_dense6, rate=0.5, noise_shape=None, seed=None, training=train_key, name=None)\n",
    "    \n",
    "    # Logits Layer\n",
    "    before_output = tf.layers.dense(inputs = result_dense6_drop, units=2, activation = None, use_bias=True,\n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                    bias_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    flat_output = tf.nn.softmax(before_output)\n",
    "    \n",
    "\n",
    "##L2 regularization\n",
    "\n",
    "trainable_var_key = tf.GraphKeys.TRAINABLE_VARIABLES\n",
    "all_vars = tf.get_collection(key=trainable_var_key, scope=\"PHM\")\n",
    "l2_losses = [tf.nn.l2_loss(w) for w in all_vars]\n",
    "sum_l2_losses =tf.add_n(l2_losses)\n",
    "    \n",
    "##corss_entropy\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=before_output , labels=y_one_hot))) + 0.1*sum_l2_losses\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(before_output,1), tf.argmax(y_one_hot,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_indexfile(file_path):\n",
    "    record_fault = np.genfromtxt((file_path+\"/start_fault_list.csv\"), delimiter=',', dtype=np.int32)\n",
    "    record_file =  np.genfromtxt((file_path+\"/start_plant_list.csv\"), delimiter=',', dtype=np.int32)\n",
    "    return record_fault,record_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_train_validate(train_ori):\n",
    "    train_ori_1 = train_ori.loc[train_ori.start_label == 1,:]\n",
    "    train_ori_1.index = range(train_ori_1.shape[0])\n",
    "    train_ori_0 = train_ori.loc[train_ori.start_label == 0,:]\n",
    "    train_ori_0.index = range(train_ori_0.shape[0])\n",
    "\n",
    "    trainset_1_pick = np.random.choice(train_ori_1.shape[0], math.ceil(train_ori_1.shape[0]/8), replace=False)\n",
    "    trainset_1 = train_ori_1.drop(trainset_1_pick)\n",
    "    validate_1 = train_ori_1.loc[trainset_1_pick,:]\n",
    "\n",
    "    trainset_0_pick = np.random.choice(train_ori_0.shape[0], math.ceil(train_ori_0.shape[0]/8), replace=False)\n",
    "    trainset_0 = train_ori_0.drop(trainset_0_pick)\n",
    "    validate_0 = train_ori_0.loc[ trainset_0_pick,: ]\n",
    "    print(\"trainset_1: %s trainset_0:%s\" % (trainset_1.shape[0],trainset_0.shape[0]))\n",
    "    ##do down sample\n",
    "    if(trainset_0.shape[0]>trainset_1.shape[0]):\n",
    "        trainset_0.index = range(trainset_0.shape[0])\n",
    "        sample_pick = np.random.choice(trainset_0.shape[0],math.ceil(trainset_1.shape[0]*1.4),replace = False)\n",
    "        trainset_0 = trainset_0.loc[sample_pick,:]\n",
    "    trainset = pd.concat([trainset_1,trainset_0], axis = 0, ignore_index = True)\n",
    "    validate = pd.concat([validate_1,validate_0], axis = 0, ignore_index = True)\n",
    "    print(\"trainset: %s validate: %s\"%(trainset.shape[0], validate.shape[0]))\n",
    "    return trainset,validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def best_model(acc_validate,dir_path):\n",
    "    best_acc = max(acc_validate)\n",
    "    for index in range( len(acc_validate) ):\n",
    "        if acc_validate[index] == best_acc:\n",
    "            break\n",
    "    best_name = dir_path+\"/model\" + str(index)+\".ckpt\"\n",
    "    print(best_name)\n",
    "    return best_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def write_record(filename,record):\n",
    "    f = open(filename,\"w\")\n",
    "    f.write( str(record[0]) )\n",
    "    for index in range(1,len(record)):\n",
    "        f.write( \",\"+str(record[index]))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pred_run(read_file_path,write_file_path,epoch_times,train_batch_size,test_batch_size):\n",
    "## read file\n",
    "    record_fault, record_file = read_indexfile(read_file_path)\n",
    "    if not os.path.exists(write_file_path):\n",
    "        os.makedirs(write_file_path )\n",
    "    \n",
    "    if not os.path.exists(write_file_path + \"/pred_start\"):\n",
    "        os.makedirs(write_file_path + \"/pred_start\")\n",
    "    \n",
    "    if not os.path.exists( (write_file_path + \"/pred_start/validate\") ):\n",
    "        os.makedirs( (write_file_path + \"/pred_start/validate\") ) \n",
    "## loop run\n",
    "    ##launch model\n",
    "    ##record_usefault_file\n",
    "    vali_record = []\n",
    "    for file_index in range(0, 2):\n",
    "        \n",
    "        print(\"index: %s , total:%s, plant:%s, fault:%s\" % (file_index,len(record_fault),record_file[file_index], record_fault[file_index]))\n",
    "        train_filename = read_file_path+\"/start/trainset\"+str(record_file[file_index])+\"_fault\"+str(record_fault[file_index])+\".csv\"\n",
    "        test_filename = read_file_path+\"/start/testset\"+str(record_file[file_index])+\"_fault\"+str(record_fault[file_index])+\".csv\"\n",
    "\n",
    "        train_ori = pd.read_csv(train_filename)\n",
    "        test_ori = pd.read_csv(test_filename)\n",
    "        train_ori_one = train_ori.loc[train_ori[\"start_label\"]==1,:]\n",
    "\n",
    "        if train_ori_one.shape[0] > 199:\n",
    "\n",
    "            ## vali_record\n",
    "            vali_record.append(1)\n",
    "            vali_record_name = write_file_path+\"/pred_start/record_vali.csv\"\n",
    "            write_record(vali_record_name,vali_record)\n",
    "\n",
    "            \n",
    "\n",
    "            trainset_raw, validate_raw = create_train_validate(train_ori)\n",
    "\n",
    "            print(trainset_raw.shape)\n",
    "            trainset = trainset_raw.drop([\"time\",\"start_label\"],axis = 1)\n",
    "            validate = validate_raw.drop([\"time\",\"start_label\"],axis = 1)\n",
    "            validate_record = validate_raw\n",
    "            testset = test_ori.drop([\"time\",\"start_label\"],axis = 1)\n",
    "\n",
    "            trainset_one_ori = trainset_raw.loc[trainset_raw.start_label == 1, :]\n",
    "            trainset_one_ori.index = range(trainset_one_ori.shape[0])\n",
    "            trainset_one = trainset_one_ori.drop([\"time\", \"start_label\"], axis = 1)\n",
    "\n",
    "            testset_one_ori = test_ori.loc[test_ori.start_label == 1,:]\n",
    "            testset_one_ori.index = range(testset_one_ori.shape[0])\n",
    "            testset_one = testset_one_ori.drop([\"time\",\"start_label\"],axis = 1)\n",
    "\n",
    "            validate_one_ori = validate_raw.loc[validate_raw.start_label == 1,:]\n",
    "            validate_one_ori.index = range(validate_one_ori.shape[0])\n",
    "            validate_one = validate_one_ori.drop([\"time\",\"start_label\"],axis = 1)\n",
    "\n",
    "            best_vali_acc = 0\n",
    "            best_score = 0\n",
    "            best_gap = 0\n",
    "            best_zero_one = 0\n",
    "            best_zero_all = 0\n",
    "            best_one_all = 0\n",
    "            ##early stop record\n",
    "            record_train = []\n",
    "            record_vali = []\n",
    "            record_vali_one = []\n",
    "            record_train_one = []\n",
    "            avg_train = 0\n",
    "            avg_train_one = 0\n",
    "            avg_vali = 0\n",
    "            avg_vali_one =0\n",
    "            \n",
    "            ##initial model\n",
    "            config = tf.ConfigProto()\n",
    "            config.gpu_options.allocator_type = 'BFC'\n",
    "            with tf.Session(config = config) as sess:\n",
    "            #sess = tf.InteractiveSession()\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                ##train\n",
    "                ##model saver path\n",
    "                dir_name = \"/plant\"+str(record_file[file_index])+\"fault\"+str(record_fault[file_index])\n",
    "                dir_path = write_file_path + \"/start_model_saver\"+dir_name\n",
    "                if os.path.exists(dir_path):\n",
    "                    shutil.rmtree(dir_path)\n",
    "                os.makedirs(dir_path)\n",
    "                saver = tf.train.Saver()\n",
    "                \n",
    "                ##start training\n",
    "                for epoch in range(epoch_times):\n",
    "                    ##training\n",
    "                    index_r = np.random.choice(trainset.shape[0],trainset.shape[0],replace = False)\n",
    "                    for index in range( math.ceil(trainset.shape[0]/train_batch_size)):\n",
    "                        if index < (math.ceil(trainset.shape[0]/train_batch_size)-1):\n",
    "                            batch_index = index_r[ [ i for i in range(index*train_batch_size,(index+1)*train_batch_size) ] ]\n",
    "                            input_train = trainset.iloc[ batch_index,: ]\n",
    "                            input_label = trainset_raw.start_label[batch_index]\n",
    "                            train_step.run(feed_dict={x: input_train.values, y_: input_label.values,train_key: True})\n",
    "                        else:\n",
    "                            batch_index = index_r[ [ i for i in range(index*train_batch_size, len(index_r) )] ]\n",
    "                            input_train = trainset.iloc[ batch_index,: ]\n",
    "                            input_label = trainset_raw.start_label[batch_index]\n",
    "                            train_step.run(feed_dict={x: input_train.values, y_: input_label.values,\n",
    "                                                      train_key: True})\n",
    "                            \n",
    "                    ##test testing set\n",
    "                    acc_test_all = 0\n",
    "                    for index in range(math.ceil(testset.shape[0]/train_batch_size)):\n",
    "                        if index < (math.ceil(testset.shape[0]/train_batch_size)-1):\n",
    "                            batch_index = [ i for i in range(index*train_batch_size,(index+1)*train_batch_size) ]\n",
    "                            input_train = testset.iloc[ batch_index,: ]\n",
    "                            input_label = test_ori.start_label[batch_index]\n",
    "                            acc_vali_part = sess.run([accuracy],\n",
    "                                                      feed_dict={x: input_train.values, y_: input_label.values,train_key: False})                                     \n",
    "                            acc_test_all = acc_test_all + acc_vali_part[0]*train_batch_size\n",
    "                        else:\n",
    "                            batch_index = [ i for i in range(index*train_batch_size,testset.shape[0]) ]\n",
    "                            input_train = testset.iloc[ batch_index,: ]\n",
    "                            input_label = test_ori.start_label[batch_index]\n",
    "                            acc_vali_part = sess.run([accuracy],\n",
    "                                                    feed_dict={x: input_train.values, y_: input_label.values,train_key: False})                            \n",
    "                            acc_test_all = acc_test_all + acc_vali_part[0]*len(batch_index)\n",
    "\n",
    "                    acc_test_all = acc_test_all / testset.shape[0]\n",
    "\n",
    "                    ##test testing set in label one\n",
    "                    acc_test_one_all = 0\n",
    "                    if(testset_one.shape[0]>0):\n",
    "                        for index in range(math.ceil(testset_one.shape[0]/train_batch_size)):\n",
    "                            if index < (math.ceil(testset_one.shape[0]/train_batch_size)-1):\n",
    "                                batch_index = [ i for i in range(index*train_batch_size,(index+1)*train_batch_size) ]\n",
    "                                input_train = testset_one.iloc[ batch_index,: ]\n",
    "                                input_label = testset_one_ori.start_label[batch_index]\n",
    "                                acc_vali_part = sess.run([accuracy],\n",
    "                                                          feed_dict={x: input_train.values, y_: input_label.values,train_key: False})                                     \n",
    "                                acc_test_one_all = acc_test_one_all + acc_vali_part[0]*train_batch_size\n",
    "                            else:\n",
    "                                batch_index = [ i for i in range(index*train_batch_size,testset_one.shape[0]) ]\n",
    "                                input_train = testset_one.iloc[ batch_index,: ]\n",
    "                                input_label = testset_one_ori.start_label[batch_index]\n",
    "                                acc_vali_part = sess.run([accuracy],\n",
    "                                                        feed_dict={x: input_train.values, y_: input_label.values,train_key: False})                            \n",
    "                                acc_test_one_all = acc_test_one_all + acc_vali_part[0]*len(batch_index)\n",
    "\n",
    "                        acc_test_one_all = acc_test_one_all / testset_one.shape[0]\n",
    "                    \n",
    "                    #test validating set\n",
    "                    acc_vali_all = 0\n",
    "                    for index in range(math.ceil(validate.shape[0]/train_batch_size)):\n",
    "                        if index < (math.ceil(validate.shape[0]/train_batch_size)-1):\n",
    "                            batch_index = [ i for i in range(index*train_batch_size,(index+1)*train_batch_size) ]\n",
    "                            input_train = validate.iloc[ batch_index,: ]\n",
    "                            input_label = validate_raw.start_label[batch_index]\n",
    "                            acc_vali_part = sess.run([accuracy],\n",
    "                                                      feed_dict={x: input_train.values, y_: input_label.values,train_key: False})                                     \n",
    "                            acc_vali_all = acc_vali_all + acc_vali_part[0]*train_batch_size\n",
    "                        else:\n",
    "                            batch_index = [ i for i in range(index*train_batch_size,validate.shape[0]) ]\n",
    "                            input_train = validate.iloc[ batch_index,: ]\n",
    "                            input_label = validate_raw.start_label[batch_index]\n",
    "                            acc_vali_part = sess.run([accuracy],\n",
    "                                                    feed_dict={x: input_train.values, y_: input_label.values,train_key: False})                            \n",
    "                            acc_vali_all = acc_vali_all + acc_vali_part[0]*len(batch_index)\n",
    "\n",
    "                    acc_vali_all = acc_vali_all / validate.shape[0]\n",
    "                    record_vali.append(acc_vali_all)\n",
    "                    \n",
    "                    #test validating set in label one\n",
    "                    acc_vali_one_all = 0\n",
    "                    if (validate_one.shape[0]>0):\n",
    "                        for index in range(math.ceil(validate_one.shape[0]/train_batch_size)):\n",
    "                            if index < (math.ceil(validate_one.shape[0]/train_batch_size)-1):\n",
    "                                batch_index = [ i for i in range(index*train_batch_size,(index+1)*train_batch_size) ]\n",
    "                                input_train = validate_one.iloc[ batch_index,: ]\n",
    "                                input_label = validate_one_ori.start_label[batch_index]\n",
    "                                acc_vali_part = sess.run([accuracy],\n",
    "                                                          feed_dict={x: input_train.values, y_: input_label.values,train_key: False})                                     \n",
    "                                acc_vali_one_all = acc_vali_one_all + acc_vali_part[0]*train_batch_size\n",
    "                            else:\n",
    "                                batch_index = [ i for i in range(index*train_batch_size,validate_one.shape[0]) ]\n",
    "                                input_train = validate_one.iloc[ batch_index,: ]\n",
    "                                input_label = validate_one_ori.start_label[batch_index]\n",
    "                                acc_vali_part = sess.run([accuracy],\n",
    "                                                        feed_dict={x: input_train.values, y_: input_label.values,train_key: False})                            \n",
    "                                acc_vali_one_all = acc_vali_one_all + acc_vali_part[0]*len(batch_index)\n",
    "\n",
    "                        acc_vali_one_all = acc_vali_one_all / validate_one.shape[0]\n",
    "                    record_vali_one.append(acc_vali_one_all)\n",
    "                    \n",
    "                    ##test training set\n",
    "                    acc_train_all = 0\n",
    "                    for index in range(math.ceil(trainset.shape[0]/train_batch_size)):\n",
    "                        if index < (math.ceil(trainset.shape[0]/train_batch_size)-1):\n",
    "                            batch_index = [ i for i in range(index*train_batch_size,(index+1)*train_batch_size) ]\n",
    "                            input_train = trainset.iloc[ batch_index,: ]\n",
    "                            input_label = trainset_raw.start_label[batch_index]\n",
    "                            acc_train_part = sess.run([accuracy],\n",
    "                                                      feed_dict={x: input_train.values, y_: input_label.values,train_key: False})                                     \n",
    "                            acc_train_all = acc_train_all + acc_train_part[0]*train_batch_size\n",
    "                        else:\n",
    "                            batch_index = [ i for i in range(index*train_batch_size,trainset.shape[0]) ]\n",
    "                            input_train = trainset.iloc[ batch_index,: ]\n",
    "                            input_label = trainset_raw.start_label[batch_index]\n",
    "                            acc_train_part = sess.run([accuracy],\n",
    "                                                    feed_dict={x: input_train.values, y_: input_label.values,train_key: False})                            \n",
    "                            acc_train_all = acc_train_all + acc_train_part[0]*len(batch_index)\n",
    "                    acc_train_all = acc_train_all / trainset.shape[0]\n",
    "                    record_train.append(acc_train_all)\n",
    "\n",
    "                    #test training set in label one\n",
    "                    acc_train_one_all = 0\n",
    "                    for index in range(math.ceil(trainset_one.shape[0]/train_batch_size)):\n",
    "                        if index < (math.ceil(trainset_one.shape[0]/train_batch_size)-1):\n",
    "                            batch_index = [ i for i in range(index*train_batch_size,(index+1)*train_batch_size) ]\n",
    "                            input_train = trainset_one.iloc[ batch_index,: ]\n",
    "                            input_label = trainset_one_ori.start_label[batch_index]\n",
    "                            acc_train_part = sess.run([accuracy],\n",
    "                                                      feed_dict={x: input_train.values, y_: input_label.values,train_key: False})                                     \n",
    "                            acc_train_one_all = acc_train_one_all + acc_train_part[0]*train_batch_size\n",
    "                        else:\n",
    "                            batch_index = [ i for i in range(index*train_batch_size,trainset_one.shape[0]) ]\n",
    "                            input_train = trainset_one.iloc[ batch_index,: ]\n",
    "                            input_label = trainset_one_ori.start_label[batch_index]\n",
    "                            acc_train_part = sess.run([accuracy],\n",
    "                                                    feed_dict={x: input_train.values, y_: input_label.values,train_key: False})                            \n",
    "                            acc_train_one_all = acc_train_one_all + acc_train_part[0]*len(batch_index)\n",
    "                            \n",
    "                    ##calculate the acc in label zero\n",
    "                    acc_train_one_all = acc_train_one_all / trainset_one.shape[0]\n",
    "                    record_train_one.append(acc_train_one_all)\n",
    "                    acc_vali_zero_all = (acc_vali_all*validate.shape[0] - acc_vali_one_all*validate_one.shape[0])/(validate.shape[0]-validate_one.shape[0])\n",
    "                    zero_one = acc_vali_zero_all * acc_vali_one_all\n",
    "\n",
    "                    acc_test_zero_all = (acc_test_all*testset.shape[0] - acc_test_one_all *testset_one.shape[0]) / (testset.shape[0] - testset_one.shape[0])\n",
    "                    now_score = validate_one.shape[0]*10*acc_vali_one_all - (validate.shape[0]-validate_one.shape[0])*(1-acc_vali_all)*0.1 \n",
    "                    print(\"epoch %d, vali: %.3g, vali_o: %.3g zero: %.3g *: %.3g train: %.3g test: %.3g test_o: %.3g zero: %.3g\"%(epoch, acc_vali_all, acc_vali_one_all,acc_vali_zero_all, zero_one, acc_train_all, acc_test_all, acc_test_one_all, acc_test_zero_all))\n",
    "\n",
    "\n",
    "                    ##record the model\n",
    "                    if (epoch > 3 ):\n",
    "                        if(epoch > 5):\n",
    "                            ##early stop\n",
    "                            avg_train_now = sum(record_train[(epoch-4):(epoch+1)])/5\n",
    "                            avg_vali_now = sum(record_vali[(epoch-4):(epoch+1)])/5\n",
    "                            avg_vali_one_now = sum(record_vali_one[(epoch-5):(epoch+1)])\n",
    "\n",
    "                        #if (now_score > best_score and acc_vali_all > best_vali_acc):\n",
    "                        #if( (acc_vali_all > best_vali_acc and zero_one > best_zero_one) or (acc_vali_all  >0.85 and acc_vali_all> acc_train_all)):\n",
    "                        ##if( acc_vali_all > best_vali_acc and zero_one > best_zero_one and acc_vali_zero_all >best_zero_all ):\n",
    "                        ##if(  zero_one > best_zero_one and acc_vali_zero_all >best_zero_all ):\n",
    "                        if(  zero_one > best_zero_one and acc_vali_all >best_vali_acc ):\n",
    "                            best_zero_one = zero_one\n",
    "                            best_vali_acc = acc_vali_all\n",
    "                            best_zero_all = acc_vali_zero_all\n",
    "                            best_one_all = acc_vali_one_all\n",
    "                            if(epoch > 5):\n",
    "                                avg_trian = sum(record_train[(epoch-4):(epoch+1)])/5\n",
    "                                avg_vali = sum(record_vali[(epoch-4):(epoch+1)])/5\n",
    "                                avg_vali_one = sum(record_vali_one[(epoch-5):(epoch+1)])\n",
    "                            print(\"best one !!!!\")\n",
    "                            best_score = now_score\n",
    "                            if os.path.exists(dir_path):\n",
    "                                shutil.rmtree(dir_path)\n",
    "                            os.makedirs(dir_path)\n",
    "                            model_name = dir_path+\"/model.ckpt\"\n",
    "                            saver.save(sess, model_name)\n",
    "\n",
    "                        if(epoch > 5):\n",
    "                            \"\"\"\n",
    "                            if( (avg_vali_one_now/avg_vali_one)<0.97):\n",
    "                                print(\"early stop vali one\")\n",
    "                                break\n",
    "\n",
    "                            if((avg_vali_now/avg_vali) < 0.97):\n",
    "                                print(\"early stop vali\")\n",
    "                                break\n",
    "                            \n",
    "                            if(avg_train_now>0.85):\n",
    "                                if((avg_vali_now/avg_train_now)<0.96):\n",
    "                                    print(\"early stop train > vali so far\")\n",
    "                                    break\n",
    "                            \"\"\"\n",
    "                            if(avg_train_now > 0.99):\n",
    "                                print(\"overfitting\")\n",
    "                                break\n",
    "                            \n",
    "\n",
    "                ##test\n",
    "                ##load the best model\n",
    "                saver.restore(sess,  model_name )\n",
    "                ##predict validating set\n",
    "                pred = []\n",
    "                for i in range( math.ceil(validate.shape[0]/test_batch_size )):\n",
    "                    if((i+1)*train_batch_size < validate.shape[0]): \n",
    "                        temp_pred = sess.run([flat_output], feed_dict={\n",
    "                            x: validate.iloc[ (i*train_batch_size) : ((i+1)*train_batch_size),: ].values,train_key: False})\n",
    "                    else:\n",
    "                        ##pdb.set_trace()\n",
    "                        temp_pred = sess.run([flat_output], feed_dict={\n",
    "                            x: validate.iloc[ (i*train_batch_size) : testset.shape[0] ].values,train_key: False})\n",
    "                    pred.extend(temp_pred[0])\n",
    "                zero_list = []\n",
    "                one_list = []\n",
    "                for array in pred:\n",
    "                    zero_list.append(array[0])\n",
    "                    one_list.append(array[1])\n",
    "\n",
    "                df = {\"zero\":zero_list, \"one\":one_list, \"true\": validate_raw.start_label, \"time\":validate_raw.time}\n",
    "                pred_data = pd.DataFrame(df)\n",
    "                pred_data_name = write_file_path + \"/pred_start/validate/start_vali_5conv\"+str(record_file[file_index])+\"_fault\"+str(record_fault[file_index])+\".csv\"\n",
    "                pred_data.to_csv(pred_data_name )\n",
    "\n",
    "                ##predict testing set  \n",
    "                pred = []\n",
    "                for i in range( math.ceil(testset.shape[0]/test_batch_size )):\n",
    "                    if((i+1)*train_batch_size < testset.shape[0]): \n",
    "                        temp_pred = sess.run([flat_output], feed_dict={\n",
    "                            x: testset.iloc[ (i*train_batch_size) : ((i+1)*train_batch_size),: ].values,train_key: False})\n",
    "                    else:\n",
    "                        temp_pred = sess.run([flat_output], feed_dict={\n",
    "                            x: testset.iloc[ (i*train_batch_size) : testset.shape[0] ].values,train_key: False})\n",
    "                    pred.extend(temp_pred[0])\n",
    "\n",
    "                zero_list = []\n",
    "                one_list = []\n",
    "                for array in pred:\n",
    "                    zero_list.append(array[0])\n",
    "                    one_list.append(array[1])\n",
    "                df = {\"zero\":zero_list, \"one\":one_list, \"true\": test_ori.start_label, \"time\": test_ori.time}\n",
    "                pred_data = pd.DataFrame(df)\n",
    "                pred_data_name = write_file_path + \"/pred_start/start_pred_5conv\"+str(record_file[file_index])+\"_fault\"+str(record_fault[file_index])+\".csv\"\n",
    "                pred_data.to_csv(pred_data_name,index = False,sep = \",\" )\n",
    "            sess.close()\n",
    "\n",
    "        else:\n",
    "            vali_record.append(0)\n",
    "            vali_record_name = write_file_path+\"/pred_start/record_vali.csv\"\n",
    "            write_record(vali_record_name,vali_record)\n",
    "    vali_record_name = write_file_path+\"/pred_start/record_vali.csv\"\n",
    "    write_record(vali_record_name,vali_record)\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0 , total:135, plant:1, fault:1\n",
      "trainset_1: 4760 trainset_0:63644\n",
      "trainset: 11424 validate: 9774\n",
      "(11424, 1523)\n",
      "epoch 0, vali: 0.718, vali_o: 0.852 zero: 0.708 *: 0.603 train: 0.775 test: 0.591 test_o: 0.949 zero: 0.569\n",
      "epoch 1, vali: 0.767, vali_o: 0.91 zero: 0.756 *: 0.688 train: 0.833 test: 0.564 test_o: 0.979 zero: 0.538\n",
      "epoch 2, vali: 0.795, vali_o: 0.927 zero: 0.786 *: 0.728 train: 0.852 test: 0.556 test_o: 0.984 zero: 0.53\n",
      "epoch 3, vali: 0.897, vali_o: 0.698 zero: 0.912 *: 0.636 train: 0.838 test: 0.672 test_o: 0.892 zero: 0.658\n",
      "epoch 4, vali: 0.855, vali_o: 0.891 zero: 0.852 *: 0.759 train: 0.879 test: 0.591 test_o: 0.977 zero: 0.567\n",
      "best one !!!!\n",
      "index: 1 , total:135, plant:1, fault:2\n",
      "trainset_1: 3569 trainset_0:63684\n",
      "trainset: 8566 validate: 9608\n",
      "(8566, 1523)\n",
      "epoch 0, vali: 0.835, vali_o: 0.863 zero: 0.833 *: 0.719 train: 0.855 test: 0.792 test_o: 0.648 zero: 0.792\n",
      "epoch 1, vali: 0.879, vali_o: 0.912 zero: 0.877 *: 0.8 train: 0.902 test: 0.964 test_o: 0.574 zero: 0.965\n"
     ]
    }
   ],
   "source": [
    "read_file_path = \"/media/joeytu/82F6C88FF6C8853F/PHM2017/PHM_nosample/file\"\n",
    "write_file_path = \"/media/joeytu/82F6C88FF6C8853F/PHM2017/20170831DEMO/file\"\n",
    "epoch_times = 5\n",
    "train_batch_size = 300\n",
    "test_batch_size = 300\n",
    "pred_run(read_file_path ,write_file_path,epoch_times, train_batch_size, test_batch_size)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
